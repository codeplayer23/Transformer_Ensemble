{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sKJYGFaBnYHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Ensemble Training Script: RoBERTa + DistilBERT + Meta-Model\n",
        "# =====================\n",
        "import os\n",
        "import lzma\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from torch.optim import AdamW\n",
        "import joblib\n",
        "\n",
        "# =====================\n",
        "# Config\n",
        "# =====================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "MAX_LEN = 256\n",
        "\n",
        "# =====================\n",
        "# Load dataset\n",
        "# =====================\n",
        "json_path = \"2023_processed.json.xz\"\n",
        "with lzma.open(json_path, 'rt') as f:\n",
        "    data = json.load(f)\n",
        "df = pd.DataFrame(data).sample(n=10000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Keep only rows with maintext and sentiment\n",
        "df = df.dropna(subset=[\"maintext\", \"sentiment\"])\n",
        "\n",
        "# Convert sentiment dict to numeric label\n",
        "def extract_label(sentiment_dict):\n",
        "    if not isinstance(sentiment_dict, dict):\n",
        "        return 1  # neutral fallback\n",
        "    labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    values = [sentiment_dict.get(l, 0) for l in labels]\n",
        "    return int(values.index(max(values)))\n",
        "\n",
        "df['label'] = df['sentiment'].apply(extract_label)\n",
        "\n",
        "texts = df['maintext'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# =====================\n",
        "# Tokenizers & Models\n",
        "# =====================\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "distil_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3).to(device)\n",
        "distil_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3).to(device)\n",
        "\n",
        "# =====================\n",
        "# Tokenization helper\n",
        "# =====================\n",
        "def tokenize_texts(tokenizer, texts):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "\n",
        "def create_loader(texts, labels, tokenizer):\n",
        "    enc = tokenize_texts(tokenizer, texts)\n",
        "    dataset = Dataset.from_dict({\n",
        "        'input_ids': enc['input_ids'],\n",
        "        'attention_mask': enc['attention_mask'],\n",
        "        'labels': torch.tensor(labels)\n",
        "    })\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
        "\n",
        "train_loader_roberta = create_loader(train_texts, train_labels, roberta_tokenizer)\n",
        "test_loader_roberta = create_loader(test_texts, test_labels, roberta_tokenizer)\n",
        "train_loader_distil = create_loader(train_texts, train_labels, distil_tokenizer)\n",
        "test_loader_distil = create_loader(test_texts, test_labels, distil_tokenizer)\n",
        "\n",
        "# =====================\n",
        "# Training helper\n",
        "# =====================\n",
        "def train_model(model, train_loader, epochs=EPOCHS):\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in loop:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "    return model\n",
        "\n",
        "# Train both models\n",
        "print(\"Training RoBERTa for 10 epochs...\")\n",
        "roberta_model = train_model(roberta_model, train_loader_roberta)\n",
        "\n",
        "print(\"Training DistilBERT for 10 epochs...\")\n",
        "distil_model = train_model(distil_model, train_loader_distil)\n",
        "\n",
        "# =====================\n",
        "#  Generate predictions for meta-model\n",
        "# =====================\n",
        "def get_preds(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            all_preds.append(probs)\n",
        "    return np.vstack(all_preds)\n",
        "\n",
        "X_train_meta = np.hstack([\n",
        "    get_preds(roberta_model, train_loader_roberta),\n",
        "    get_preds(distil_model, train_loader_distil)\n",
        "])\n",
        "X_test_meta = np.hstack([\n",
        "    get_preds(roberta_model, test_loader_roberta),\n",
        "    get_preds(distil_model, test_loader_distil)\n",
        "])\n",
        "y_train_meta = np.array(train_labels)\n",
        "y_test_meta = np.array(test_labels)\n",
        "\n",
        "# =====================\n",
        "# Train meta-model\n",
        "# =====================\n",
        "meta_model = LogisticRegression(max_iter=2000)\n",
        "meta_model.fit(X_train_meta, y_train_meta)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_meta = meta_model.predict(X_test_meta)\n",
        "print(\"Meta-Model Accuracy:\", accuracy_score(y_test_meta, y_pred_meta))\n",
        "print(classification_report(y_test_meta, y_pred_meta, target_names=[\"Negative\",\"Neutral\",\"Positive\"]))\n",
        "\n",
        "# =====================\n",
        "# Save models & meta-model\n",
        "# =====================\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "roberta_model.save_pretrained(\"saved_models/roberta\")\n",
        "distil_model.save_pretrained(\"saved_models/distilbert\")\n",
        "roberta_tokenizer.save_pretrained(\"saved_models/roberta\")\n",
        "distil_tokenizer.save_pretrained(\"saved_models/distilbert\")\n",
        "joblib.dump(meta_model, \"saved_models/meta_model.pkl\")\n",
        "print(\" All models and meta-model saved!\")\n",
        "\n",
        "# =====================\n",
        "# Ensemble prediction function\n",
        "# =====================\n",
        "def predict_ensemble(text: str):\n",
        "    roberta_enc = tokenize_texts(roberta_tokenizer, [text])\n",
        "    distil_enc = tokenize_texts(distil_tokenizer, [text])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        roberta_probs = torch.softmax(\n",
        "            roberta_model(roberta_enc['input_ids'].to(device), attention_mask=roberta_enc['attention_mask'].to(device)).logits,\n",
        "            dim=1\n",
        "        ).cpu().numpy()\n",
        "        distil_probs = torch.softmax(\n",
        "            distil_model(distil_enc['input_ids'].to(device), attention_mask=distil_enc['attention_mask'].to(device)).logits,\n",
        "            dim=1\n",
        "        ).cpu().numpy()\n",
        "\n",
        "    meta_input = np.hstack([roberta_probs, distil_probs])\n",
        "    pred_class = meta_model.predict(meta_input)[0]\n",
        "    pred_prob = meta_model.predict_proba(meta_input)[0][pred_class]\n",
        "\n",
        "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    return {\"label_id\": int(pred_class), \"label\": label_map[int(pred_class)], \"confidence\": float(pred_prob)}\n",
        "\n",
        "# Example\n",
        "sample_text = \"The company's revenue exceeded expectations this quarter.\"\n",
        "print(predict_ensemble(sample_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfJDPcoAIG5F",
        "outputId": "9235c08d-f1a7-48a4-ed4d-4c774d3a2126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RoBERTa for 10 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 500/500 [05:49<00:00,  1.43it/s, loss=0.336]\n",
            "Training Epoch 2/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.118]\n",
            "Training Epoch 3/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.269]\n",
            "Training Epoch 4/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.182]\n",
            "Training Epoch 5/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.452]\n",
            "Training Epoch 6/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.21]\n",
            "Training Epoch 7/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.0273]\n",
            "Training Epoch 8/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.00877]\n",
            "Training Epoch 9/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.00703]\n",
            "Training Epoch 10/10: 100%|██████████| 500/500 [05:52<00:00,  1.42it/s, loss=0.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DistilBERT for 10 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 500/500 [02:59<00:00,  2.78it/s, loss=0.489]\n",
            "Training Epoch 2/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.294]\n",
            "Training Epoch 3/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.54]\n",
            "Training Epoch 4/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.0406]\n",
            "Training Epoch 5/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.0181]\n",
            "Training Epoch 6/10: 100%|██████████| 500/500 [02:56<00:00,  2.84it/s, loss=0.0974]\n",
            "Training Epoch 7/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.313]\n",
            "Training Epoch 8/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.0439]\n",
            "Training Epoch 9/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.224]\n",
            "Training Epoch 10/10: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.00184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-Model Accuracy: 0.537\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       487\n",
            "     Neutral       0.00      0.00      0.00       439\n",
            "    Positive       0.54      1.00      0.70      1074\n",
            "\n",
            "    accuracy                           0.54      2000\n",
            "   macro avg       0.18      0.33      0.23      2000\n",
            "weighted avg       0.29      0.54      0.38      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All models and meta-model saved!\n",
            "{'label_id': 2, 'label': 'Positive', 'confidence': 0.5365825570676341}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import joblib\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =====================\n",
        "# 1️⃣ Load saved models\n",
        "# =====================\n",
        "roberta_path = \"saved_models/roberta\"\n",
        "distil_path = \"saved_models/distilbert\"\n",
        "meta_path = \"saved_models/meta_model.pkl\"\n",
        "\n",
        "if not (os.path.exists(roberta_path) and os.path.exists(distil_path) and os.path.exists(meta_path)):\n",
        "    raise FileNotFoundError(\"Saved models not found. Please train and save them first.\")\n",
        "\n",
        "# Load tokenizers & models\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_path)\n",
        "distil_tokenizer = AutoTokenizer.from_pretrained(distil_path)\n",
        "\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_path).to(device)\n",
        "distil_model = AutoModelForSequenceClassification.from_pretrained(distil_path).to(device)\n",
        "\n",
        "# Load meta-model\n",
        "meta_model = joblib.load(meta_path)\n",
        "\n",
        "# =====================\n",
        "# 2️⃣ Prediction function\n",
        "# =====================\n",
        "def tokenize_texts(tokenizer, texts, max_len=256):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
        "\n",
        "def predict_ensemble(text: str):\n",
        "    roberta_enc = tokenize_texts(roberta_tokenizer, [text])\n",
        "    distil_enc = tokenize_texts(distil_tokenizer, [text])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        roberta_probs = torch.softmax(\n",
        "            roberta_model(roberta_enc['input_ids'].to(device),\n",
        "                          attention_mask=roberta_enc['attention_mask'].to(device)).logits,\n",
        "            dim=1\n",
        "        ).cpu().numpy()\n",
        "\n",
        "        distil_probs = torch.softmax(\n",
        "            distil_model(distil_enc['input_ids'].to(device),\n",
        "                         attention_mask=distil_enc['attention_mask'].to(device)).logits,\n",
        "            dim=1\n",
        "        ).cpu().numpy()\n",
        "\n",
        "    meta_input = np.hstack([roberta_probs, distil_probs])\n",
        "    pred_class = meta_model.predict(meta_input)[0]\n",
        "    pred_prob = meta_model.predict_proba(meta_input)[0][pred_class]\n",
        "\n",
        "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    return {\"label_id\": int(pred_class), \"label\": label_map[int(pred_class)], \"confidence\": float(pred_prob)}\n",
        "\n",
        "# =====================\n",
        "# 3️⃣ Predict sample paragraph\n",
        "# =====================\n",
        "sample_text = \"\"\"Global markets reacted cautiously as multiple central banks signaled diverging monetary policies.\n",
        "While the Federal Reserve indicated a potential slowdown in interest rate hikes, citing moderating inflation,\n",
        "the European Central Bank emphasized persistent price pressures in the eurozone, leaving investors uncertain\n",
        "about future liquidity conditions. Meanwhile, tech giants reported robust quarterly earnings, beating analyst\n",
        "expectations, yet supply chain disruptions and rising raw material costs cast a shadow over projected profit margins.\n",
        "In emerging markets, currency fluctuations and geopolitical tensions further complicated cross-border investment strategies.\n",
        "Analysts warn that although short-term gains are visible in certain sectors, systemic risks may offset these benefits,\n",
        "making the overall outlook highly volatile and sentimentally ambiguous.\"\"\"\n",
        "\n",
        "prediction = predict_ensemble(sample_text)\n",
        "print(\"\\nSample Paragraph Prediction:\")\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIOJM0IAJyjw",
        "outputId": "b9d4d044-06e3-4ac5-824e-059478a33fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Paragraph Prediction:\n",
            "{'label_id': 2, 'label': 'Positive', 'confidence': 0.5384260045009355}\n"
          ]
        }
      ]
    }
  ]
}